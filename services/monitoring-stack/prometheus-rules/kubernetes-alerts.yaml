apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: kubernetes-alerts
  namespace: monitoring
  labels:
    release: kube-prometheus-stack
spec:
  groups:
    - name: kubernetes-alerts
      rules:
        - alert: KubeAPIServerDown
          expr: up{job="kubernetes-apiservers"} == 0
          for: 2m
          labels:
            severity: critical
          annotations:
            summary: "Kubernetes API server is down"
            description: "API server {{ $labels.instance }} has been unreachable for more than 2 minutes."

        - alert: EtcdMemberDown
          expr: up{job="etcd"} == 0
          for: 2m
          labels:
            severity: critical
          annotations:
            summary: "etcd member is down"
            description: "etcd member {{ $labels.instance }} has been unreachable for more than 2 minutes."

        - alert: EtcdHighLatency
          expr: histogram_quantile(0.99, rate(etcd_disk_wal_fsync_duration_seconds_bucket[5m])) > 1
          for: 15m
          labels:
            severity: warning
          annotations:
            summary: "etcd high WAL fsync latency"
            description: "etcd WAL fsync 99th percentile latency is {{ $value | printf \"%.3f\" }}s on {{ $labels.instance }}. Harvester/Longhorn storage adds ~100-200ms baseline latency. Kubernetes 1.34 gRPC-go library also logs benign connection cycling warnings at ~1/min that do not indicate data loss."

        - alert: EtcdHighCommitDuration
          expr: histogram_quantile(0.99, rate(etcd_disk_backend_commit_duration_seconds_bucket[5m])) > 0.5
          for: 15m
          labels:
            severity: warning
          annotations:
            summary: "etcd high backend commit duration"
            description: "etcd backend commit 99th percentile duration is {{ $value | printf \"%.3f\" }}s on {{ $labels.instance }}. Harvester/Longhorn storage adds ~100-200ms baseline latency vs local NVMe."

        - alert: KubePodCrashLooping
          expr: increase(kube_pod_container_status_restarts_total[1h]) > 5
          for: 15m
          labels:
            severity: warning
          annotations:
            summary: "Pod {{ $labels.namespace }}/{{ $labels.pod }} is crash looping"
            description: "Pod {{ $labels.namespace }}/{{ $labels.pod }} has restarted {{ $value | printf \"%.0f\" }} times in the last hour."

        - alert: KubePodNotReady
          expr: kube_pod_status_phase{phase=~"Pending|Unknown"} == 1
          for: 15m
          labels:
            severity: warning
          annotations:
            summary: "Pod {{ $labels.namespace }}/{{ $labels.pod }} is not ready"
            description: "Pod {{ $labels.namespace }}/{{ $labels.pod }} has been in {{ $labels.phase }} state for more than 15 minutes."

        - alert: KubeDeploymentReplicasMismatch
          expr: kube_deployment_spec_replicas != kube_deployment_status_replicas_ready
          for: 15m
          labels:
            severity: warning
          annotations:
            summary: "Deployment {{ $labels.namespace }}/{{ $labels.deployment }} replicas mismatch"
            description: "Deployment {{ $labels.namespace }}/{{ $labels.deployment }} has {{ $value }} ready replicas, expected {{ printf `kube_deployment_spec_replicas{namespace=\"%s\",deployment=\"%s\"}` $labels.namespace $labels.deployment | query | first | value }}."

        - alert: KubeDaemonSetNotScheduled
          expr: kube_daemonset_status_desired_number_scheduled - kube_daemonset_status_current_number_scheduled > 0
          for: 10m
          labels:
            severity: warning
          annotations:
            summary: "DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset }} not fully scheduled"
            description: "DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset }} has {{ $value }} pods not scheduled."

        - alert: KubeNodeNotReady
          expr: kube_node_status_condition{condition="Ready",status="true"} == 0
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: "Node {{ $labels.node }} is not ready"
            description: "Node {{ $labels.node }} has been in NotReady state for more than 5 minutes."

        - alert: KubeletDown
          expr: up{job="kubelet"} == 0
          for: 2m
          labels:
            severity: critical
          annotations:
            summary: "Kubelet is down on {{ $labels.node }}"
            description: "Kubelet on {{ $labels.node }} has been unreachable for more than 2 minutes."
