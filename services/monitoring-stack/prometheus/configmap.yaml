apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
  namespace: monitoring
  labels:
    app: prometheus
data:
  prometheus.yml: |
    global:
      scrape_interval: 30s
      evaluation_interval: 30s
      scrape_timeout: 10s

    alerting:
      alertmanagers:
        - static_configs:
            - targets:
                - alertmanager.monitoring.svc:9093

    rule_files:
      - /etc/prometheus/rules.yml

    scrape_configs:
      # 1. Self-monitoring
      - job_name: "prometheus"
        static_configs:
          - targets: ["localhost:9090"]

      # 2. Kubernetes API servers
      - job_name: "kubernetes-apiservers"
        kubernetes_sd_configs:
          - role: endpoints
        scheme: https
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        authorization:
          credentials_file: /var/run/secrets/kubernetes.io/serviceaccount/token
        relabel_configs:
          - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
            action: keep
            regex: default;kubernetes;https

      # 3. Kubelet metrics
      - job_name: "kubelet"
        kubernetes_sd_configs:
          - role: node
        scheme: https
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          insecure_skip_verify: true
        authorization:
          credentials_file: /var/run/secrets/kubernetes.io/serviceaccount/token
        relabel_configs:
          - action: labelmap
            regex: __meta_kubernetes_node_label_(.+)

      # 4. cAdvisor (container metrics)
      - job_name: "cadvisor"
        kubernetes_sd_configs:
          - role: node
        scheme: https
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          insecure_skip_verify: true
        authorization:
          credentials_file: /var/run/secrets/kubernetes.io/serviceaccount/token
        metrics_path: /metrics/cadvisor
        relabel_configs:
          - action: labelmap
            regex: __meta_kubernetes_node_label_(.+)

      # 5. etcd metrics (exposed on dedicated HTTP port 2381 via etcd-expose-metrics=true)
      #    Port 2379 is gRPC-only; 2381 is the plain HTTP metrics endpoint (no mTLS)
      #    Uses node discovery to dynamically find control-plane nodes
      - job_name: "etcd"
        scheme: http
        kubernetes_sd_configs:
          - role: node
        relabel_configs:
          - source_labels: [__meta_kubernetes_node_labelpresent_node_role_kubernetes_io_control_plane]
            action: keep
            regex: "true"
          - source_labels: [__meta_kubernetes_node_address_InternalIP]
            action: replace
            target_label: __address__
            replacement: $1:2381
          - source_labels: [__meta_kubernetes_node_name]
            target_label: instance

      # 6. Cilium agent metrics
      # NOTE: cilium-agent uses hostNetwork and port 9962 is blocked by host firewall.
      # Requires opening port 9962/tcp on all nodes to enable direct scraping.
      # Until then, only hubble-relay metrics are available (job 6b below).
      - job_name: "cilium-agent"
        kubernetes_sd_configs:
          - role: pod
            namespaces:
              names: ["kube-system"]
        relabel_configs:
          - source_labels: [__meta_kubernetes_pod_label_k8s_app]
            action: keep
            regex: cilium
          - source_labels: [__meta_kubernetes_pod_ip]
            action: replace
            target_label: __address__
            replacement: $1:9962
          - source_labels: [__meta_kubernetes_pod_node_name]
            target_label: node

      # 6b. Hubble relay metrics (Cilium flow observability)
      - job_name: "hubble-relay"
        kubernetes_sd_configs:
          - role: endpoints
            namespaces:
              names: ["kube-system"]
        relabel_configs:
          - source_labels: [__meta_kubernetes_service_name]
            action: keep
            regex: hubble-relay-metrics
          - source_labels: [__meta_kubernetes_endpoint_port_name]
            action: keep
            regex: metrics

      # 7. CoreDNS
      - job_name: "coredns"
        kubernetes_sd_configs:
          - role: pod
            namespaces:
              names: ["kube-system"]
        relabel_configs:
          - source_labels: [__meta_kubernetes_pod_label_app_kubernetes_io_name]
            action: keep
            regex: rke2-coredns
          - source_labels: [__meta_kubernetes_pod_ip]
            action: replace
            target_label: __address__
            replacement: $1:9153

      # 8. kube-scheduler (dynamic control-plane node discovery)
      - job_name: "kube-scheduler"
        scheme: https
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          insecure_skip_verify: true
        authorization:
          credentials_file: /var/run/secrets/kubernetes.io/serviceaccount/token
        kubernetes_sd_configs:
          - role: node
        relabel_configs:
          - source_labels: [__meta_kubernetes_node_labelpresent_node_role_kubernetes_io_control_plane]
            action: keep
            regex: "true"
          - source_labels: [__meta_kubernetes_node_address_InternalIP]
            action: replace
            target_label: __address__
            replacement: $1:10259
          - source_labels: [__meta_kubernetes_node_name]
            target_label: instance

      # 9. kube-controller-manager (dynamic control-plane node discovery)
      - job_name: "kube-controller-manager"
        scheme: https
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          insecure_skip_verify: true
        authorization:
          credentials_file: /var/run/secrets/kubernetes.io/serviceaccount/token
        kubernetes_sd_configs:
          - role: node
        relabel_configs:
          - source_labels: [__meta_kubernetes_node_labelpresent_node_role_kubernetes_io_control_plane]
            action: keep
            regex: "true"
          - source_labels: [__meta_kubernetes_node_address_InternalIP]
            action: replace
            target_label: __address__
            replacement: $1:10257
          - source_labels: [__meta_kubernetes_node_name]
            target_label: instance

      # 10. Node Exporter
      - job_name: "node-exporter"
        kubernetes_sd_configs:
          - role: endpoints
        relabel_configs:
          - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name]
            action: keep
            regex: monitoring;node-exporter
          - source_labels: [__meta_kubernetes_endpoint_node_name]
            target_label: node

      # 11. kube-state-metrics
      - job_name: "kube-state-metrics"
        kubernetes_sd_configs:
          - role: endpoints
        relabel_configs:
          - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name]
            action: keep
            regex: monitoring;kube-state-metrics
          - source_labels: [__meta_kubernetes_endpoint_port_name]
            action: keep
            regex: metrics

      # 12. Annotation-based service endpoint discovery
      - job_name: "kubernetes-service-endpoints"
        kubernetes_sd_configs:
          - role: endpoints
        relabel_configs:
          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]
            action: keep
            regex: "true"
          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]
            action: replace
            target_label: __scheme__
            regex: (https?)
          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]
            action: replace
            target_label: __metrics_path__
            regex: (.+)
          - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]
            action: replace
            regex: ([^:]+)(?::\d+)?;(\d+)
            replacement: $1:$2
            target_label: __address__
          - action: labelmap
            regex: __meta_kubernetes_service_label_(.+)
          - source_labels: [__meta_kubernetes_namespace]
            action: replace
            target_label: namespace
          - source_labels: [__meta_kubernetes_service_name]
            action: replace
            target_label: service

      # 13. Annotation-based pod discovery
      - job_name: "kubernetes-pods"
        kubernetes_sd_configs:
          - role: pod
        relabel_configs:
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
            action: keep
            regex: "true"
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scheme]
            action: replace
            target_label: __scheme__
            regex: (https?)
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
            action: replace
            target_label: __metrics_path__
            regex: (.+)
          - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
            action: replace
            regex: ([^:]+)(?::\d+)?;(\d+)
            replacement: $1:$2
            target_label: __address__
          - action: labelmap
            regex: __meta_kubernetes_pod_label_(.+)
          - source_labels: [__meta_kubernetes_namespace]
            action: replace
            target_label: namespace
          - source_labels: [__meta_kubernetes_pod_name]
            action: replace
            target_label: pod
          - source_labels: [__meta_kubernetes_pod_node_name]
            action: replace
            target_label: node

      # 14. Vault metrics
      - job_name: "vault"
        metrics_path: /v1/sys/metrics
        params:
          format: ["prometheus"]
        kubernetes_sd_configs:
          - role: endpoints
            namespaces:
              names: ["vault"]
        relabel_configs:
          - source_labels: [__meta_kubernetes_service_name]
            action: keep
            regex: vault-internal
          - source_labels: [__meta_kubernetes_endpoint_port_name]
            action: keep
            regex: http

      # 15. cert-manager
      - job_name: "cert-manager"
        kubernetes_sd_configs:
          - role: endpoints
            namespaces:
              names: ["cert-manager"]
        relabel_configs:
          - source_labels: [__meta_kubernetes_service_name]
            action: keep
            regex: cert-manager
          - source_labels: [__meta_kubernetes_endpoint_port_name]
            action: keep
            regex: tcp-prometheus-servicemonitor

      # 16. Traefik ingress controller
      - job_name: "traefik"
        kubernetes_sd_configs:
          - role: pod
            namespaces:
              names: ["kube-system"]
        relabel_configs:
          - source_labels: [__meta_kubernetes_pod_label_app_kubernetes_io_name]
            action: keep
            regex: rke2-traefik
          - source_labels: [__meta_kubernetes_pod_container_port_number]
            action: keep
            regex: "9100"
          - source_labels: [__meta_kubernetes_pod_node_name]
            target_label: node

      # 17. CNPG controller manager
      - job_name: "cnpg-controller"
        kubernetes_sd_configs:
          - role: pod
            namespaces:
              names: ["cnpg-system"]
        relabel_configs:
          - source_labels: [__meta_kubernetes_pod_label_app_kubernetes_io_name]
            action: keep
            regex: cloudnative-pg
          - source_labels: [__meta_kubernetes_pod_container_port_number]
            action: keep
            regex: "8080"

      # 18. CNPG PostgreSQL instances
      - job_name: "cnpg-postgresql"
        kubernetes_sd_configs:
          - role: pod
            namespaces:
              names: ["database"]
        relabel_configs:
          - source_labels: [__meta_kubernetes_pod_label_cnpg_io_cluster]
            action: keep
            regex: .+
          - source_labels: [__meta_kubernetes_pod_container_port_number]
            action: keep
            regex: "9187"
          - source_labels: [__meta_kubernetes_pod_label_cnpg_io_cluster]
            target_label: cnpg_cluster
          - source_labels: [__meta_kubernetes_pod_label_role]
            target_label: role

      # 19. Alloy self-monitoring
      - job_name: "alloy"
        kubernetes_sd_configs:
          - role: endpoints
            namespaces:
              names: ["monitoring"]
        relabel_configs:
          - source_labels: [__meta_kubernetes_service_name]
            action: keep
            regex: alloy
          - source_labels: [__meta_kubernetes_endpoint_port_name]
            action: keep
            regex: http

      # 20. Loki self-monitoring
      - job_name: "loki"
        kubernetes_sd_configs:
          - role: endpoints
            namespaces:
              names: ["monitoring"]
        relabel_configs:
          - source_labels: [__meta_kubernetes_service_name]
            action: keep
            regex: loki
          - source_labels: [__meta_kubernetes_endpoint_port_name]
            action: keep
            regex: http

      # 21. GitLab exporter
      - job_name: "gitlab-exporter"
        kubernetes_sd_configs:
          - role: endpoints
            namespaces:
              names: ["gitlab"]
        relabel_configs:
          - source_labels: [__meta_kubernetes_service_name]
            action: keep
            regex: gitlab-gitlab-exporter
          - source_labels: [__meta_kubernetes_endpoint_port_name]
            action: keep
            regex: http-metrics

      # 22. ArgoCD metrics (controller, server, repoServer, applicationSet)
      - job_name: "argocd"
        kubernetes_sd_configs:
          - role: endpoints
            namespaces:
              names: ["argocd"]
        relabel_configs:
          - source_labels: [__meta_kubernetes_service_name]
            action: keep
            regex: argocd-.*-metrics
          - source_labels: [__meta_kubernetes_endpoint_port_name]
            action: keep
            regex: http-metrics
          - source_labels: [__meta_kubernetes_service_name]
            target_label: component
            regex: argocd-(.*)-metrics
            replacement: $1

      # 23. Argo Rollouts
      - job_name: "argo-rollouts"
        kubernetes_sd_configs:
          - role: endpoints
            namespaces:
              names: ["argo-rollouts"]
        relabel_configs:
          - source_labels: [__meta_kubernetes_service_name]
            action: keep
            regex: argo-rollouts-metrics
          - source_labels: [__meta_kubernetes_endpoint_port_name]
            action: keep
            regex: metrics

      # 24. Harbor (core, registry, exporter)
      - job_name: "harbor"
        kubernetes_sd_configs:
          - role: endpoints
            namespaces:
              names: ["harbor"]
        relabel_configs:
          - source_labels: [__meta_kubernetes_service_name]
            action: keep
            regex: harbor-(core|registry|exporter)
          - source_labels: [__meta_kubernetes_endpoint_port_name]
            action: keep
            regex: http-metrics
          - source_labels: [__meta_kubernetes_service_name]
            target_label: component
            regex: harbor-(.*)
            replacement: $1

      # 25. Keycloak (metrics served on management port 9000 in Keycloak 26+)
      - job_name: "keycloak"
        metrics_path: /metrics
        kubernetes_sd_configs:
          - role: pod
            namespaces:
              names: ["keycloak"]
        relabel_configs:
          - source_labels: [__meta_kubernetes_pod_label_app]
            action: keep
            regex: keycloak
          - source_labels: [__meta_kubernetes_pod_container_port_number]
            action: keep
            regex: "9000"

      # 26. Mattermost (requires Enterprise license for /metrics endpoint)
      - job_name: "mattermost"
        kubernetes_sd_configs:
          - role: endpoints
            namespaces:
              names: ["mattermost"]
        relabel_configs:
          - source_labels: [__meta_kubernetes_service_name]
            action: keep
            regex: mattermost-metrics
          - source_labels: [__meta_kubernetes_endpoint_port_name]
            action: keep
            regex: metrics

      # 27. Alertmanager
      - job_name: "alertmanager"
        kubernetes_sd_configs:
          - role: endpoints
            namespaces:
              names: ["monitoring"]
        relabel_configs:
          - source_labels: [__meta_kubernetes_service_name]
            action: keep
            regex: alertmanager
          - source_labels: [__meta_kubernetes_endpoint_port_name]
            action: keep
            regex: http

  rules.yml: |
    groups:
      - name: node-alerts
        rules:
          - alert: NodeDown
            expr: up{job="node-exporter"} == 0
            for: 2m
            labels:
              severity: critical
            annotations:
              summary: "Node exporter down on {{ $labels.node }}"
              description: "Node exporter on {{ $labels.node }} has been unreachable for more than 2 minutes."

          - alert: NodeHighCPU
            expr: 100 - (avg by(node) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 90
            for: 15m
            labels:
              severity: warning
            annotations:
              summary: "High CPU usage on {{ $labels.node }}"
              description: "CPU usage on {{ $labels.node }} has been above 90% for 15 minutes (current: {{ $value | printf \"%.1f\" }}%)."

          - alert: NodeHighMemory
            expr: (1 - node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes) * 100 > 90
            for: 15m
            labels:
              severity: warning
            annotations:
              summary: "High memory usage on {{ $labels.node }}"
              description: "Memory usage on {{ $labels.node }} has been above 90% for 15 minutes (current: {{ $value | printf \"%.1f\" }}%)."

          - alert: NodeDiskPressure
            expr: (1 - node_filesystem_avail_bytes{fstype!~"tmpfs|overlay"} / node_filesystem_size_bytes{fstype!~"tmpfs|overlay"}) * 100 > 85
            for: 10m
            labels:
              severity: warning
            annotations:
              summary: "Disk pressure on {{ $labels.node }}"
              description: "Filesystem {{ $labels.mountpoint }} on {{ $labels.node }} is {{ $value | printf \"%.1f\" }}% full."

          - alert: NodeDiskCritical
            expr: (1 - node_filesystem_avail_bytes{fstype!~"tmpfs|overlay"} / node_filesystem_size_bytes{fstype!~"tmpfs|overlay"}) * 100 > 95
            for: 5m
            labels:
              severity: critical
            annotations:
              summary: "Disk critically full on {{ $labels.node }}"
              description: "Filesystem {{ $labels.mountpoint }} on {{ $labels.node }} is {{ $value | printf \"%.1f\" }}% full."

      - name: kubernetes-alerts
        rules:
          - alert: KubeAPIServerDown
            expr: up{job="kubernetes-apiservers"} == 0
            for: 2m
            labels:
              severity: critical
            annotations:
              summary: "Kubernetes API server is down"
              description: "API server {{ $labels.instance }} has been unreachable for more than 2 minutes."

          - alert: EtcdMemberDown
            expr: up{job="etcd"} == 0
            for: 2m
            labels:
              severity: critical
            annotations:
              summary: "etcd member is down"
              description: "etcd member {{ $labels.instance }} has been unreachable for more than 2 minutes."

          - alert: EtcdHighLatency
            expr: histogram_quantile(0.99, rate(etcd_disk_wal_fsync_duration_seconds_bucket[5m])) > 0.5
            for: 10m
            labels:
              severity: warning
            annotations:
              summary: "etcd high WAL fsync latency"
              description: "etcd WAL fsync 99th percentile latency is {{ $value | printf \"%.3f\" }}s on {{ $labels.instance }}."

          - alert: EtcdHighCommitDuration
            expr: histogram_quantile(0.99, rate(etcd_disk_backend_commit_duration_seconds_bucket[5m])) > 0.25
            for: 10m
            labels:
              severity: warning
            annotations:
              summary: "etcd high backend commit duration"
              description: "etcd backend commit 99th percentile duration is {{ $value | printf \"%.3f\" }}s on {{ $labels.instance }}."

          - alert: KubePodCrashLooping
            expr: increase(kube_pod_container_status_restarts_total[1h]) > 5
            for: 15m
            labels:
              severity: warning
            annotations:
              summary: "Pod {{ $labels.namespace }}/{{ $labels.pod }} is crash looping"
              description: "Pod {{ $labels.namespace }}/{{ $labels.pod }} has restarted {{ $value | printf \"%.0f\" }} times in the last hour."

          - alert: KubePodNotReady
            expr: kube_pod_status_phase{phase=~"Pending|Unknown"} == 1
            for: 15m
            labels:
              severity: warning
            annotations:
              summary: "Pod {{ $labels.namespace }}/{{ $labels.pod }} is not ready"
              description: "Pod {{ $labels.namespace }}/{{ $labels.pod }} has been in {{ $labels.phase }} state for more than 15 minutes."

          - alert: KubeDeploymentReplicasMismatch
            expr: kube_deployment_spec_replicas != kube_deployment_status_ready_replicas
            for: 15m
            labels:
              severity: warning
            annotations:
              summary: "Deployment {{ $labels.namespace }}/{{ $labels.deployment }} replicas mismatch"
              description: "Deployment {{ $labels.namespace }}/{{ $labels.deployment }} has {{ $value }} ready replicas, expected {{ printf `kube_deployment_spec_replicas{namespace=\"%s\",deployment=\"%s\"}` $labels.namespace $labels.deployment | query | first | value }}."

          - alert: KubeDaemonSetNotScheduled
            expr: kube_daemonset_status_desired_number_scheduled - kube_daemonset_status_current_number_scheduled > 0
            for: 10m
            labels:
              severity: warning
            annotations:
              summary: "DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset }} not fully scheduled"
              description: "DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset }} has {{ $value }} pods not scheduled."

          - alert: KubeNodeNotReady
            expr: kube_node_status_condition{condition="Ready",status="true"} == 0
            for: 5m
            labels:
              severity: critical
            annotations:
              summary: "Node {{ $labels.node }} is not ready"
              description: "Node {{ $labels.node }} has been in NotReady state for more than 5 minutes."

          - alert: KubeletDown
            expr: up{job="kubelet"} == 0
            for: 2m
            labels:
              severity: critical
            annotations:
              summary: "Kubelet is down on {{ $labels.node }}"
              description: "Kubelet on {{ $labels.node }} has been unreachable for more than 2 minutes."

      - name: vault-alerts
        rules:
          - alert: VaultSealed
            expr: vault_core_unsealed == 0
            for: 2m
            labels:
              severity: critical
            annotations:
              summary: "Vault instance is sealed"
              description: "Vault instance {{ $labels.instance }} has been sealed for more than 2 minutes."

          - alert: VaultDown
            expr: up{job="vault"} == 0
            for: 2m
            labels:
              severity: critical
            annotations:
              summary: "Vault is down"
              description: "Vault instance {{ $labels.instance }} has been unreachable for more than 2 minutes."

          - alert: VaultLeaderLost
            expr: vault_core_active == 0 and on() count(up{job="vault"} == 1) > 0
            for: 5m
            labels:
              severity: critical
            annotations:
              summary: "Vault has no active leader"
              description: "No Vault instance is reporting as the active leader for more than 5 minutes."

      - name: certmanager-alerts
        rules:
          - alert: CertExpiringSoon
            expr: certmanager_certificate_expiration_timestamp_seconds - time() < 604800
            for: 1h
            labels:
              severity: warning
            annotations:
              summary: "Certificate {{ $labels.name }} expiring soon"
              description: "Certificate {{ $labels.name }} in namespace {{ $labels.namespace }} expires in {{ $value | humanizeDuration }}."

          - alert: CertNotReady
            expr: certmanager_certificate_ready_status{condition="True"} == 0
            for: 15m
            labels:
              severity: warning
            annotations:
              summary: "Certificate {{ $labels.name }} is not ready"
              description: "Certificate {{ $labels.name }} in namespace {{ $labels.namespace }} has been in a not-ready state for more than 15 minutes."

          - alert: CertManagerDown
            expr: up{job="cert-manager"} == 0
            for: 5m
            labels:
              severity: critical
            annotations:
              summary: "cert-manager is down"
              description: "cert-manager has been unreachable for more than 5 minutes."

      - name: gitlab-alerts
        rules:
          - alert: GitLabDown
            expr: up{job="gitlab-exporter"} == 0
            for: 5m
            labels:
              severity: critical
            annotations:
              summary: "GitLab exporter is down"
              description: "GitLab exporter has been unreachable for more than 5 minutes."

      - name: postgresql-alerts
        rules:
          - alert: PostgreSQLDown
            expr: up{job="cnpg-postgresql"} == 0
            for: 2m
            labels:
              severity: critical
            annotations:
              summary: "PostgreSQL instance is down"
              description: "PostgreSQL instance {{ $labels.instance }} (cluster: {{ $labels.cnpg_cluster }}) has been unreachable for more than 2 minutes."

          - alert: ReplicationLag
            expr: cnpg_pg_replication_lag > 30
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "PostgreSQL replication lag is high"
              description: "Replication lag for {{ $labels.cnpg_cluster }} is {{ $value | printf \"%.1f\" }}s (threshold: 30s)."

          - alert: HighConnections
            expr: sum by (cnpg_cluster) (cnpg_backends_total) / sum by (cnpg_cluster) (cnpg_pg_settings_setting{name="max_connections"}) * 100 > 80
            for: 10m
            labels:
              severity: warning
            annotations:
              summary: "PostgreSQL connections are high"
              description: "Connection usage for cluster {{ $labels.cnpg_cluster }} is at {{ $value | printf \"%.1f\" }}%."

          - alert: CNPGControllerDown
            expr: up{job="cnpg-controller"} == 0
            for: 5m
            labels:
              severity: critical
            annotations:
              summary: "CNPG controller is down"
              description: "CloudNativePG controller has been unreachable for more than 5 minutes."

      - name: monitoring-self-alerts
        rules:
          - alert: PrometheusTargetDown
            expr: up == 0
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Prometheus target {{ $labels.job }} is down"
              description: "Target {{ $labels.instance }} in job {{ $labels.job }} has been down for more than 5 minutes."

          - alert: PrometheusTSDBCompactionsFailing
            expr: increase(prometheus_tsdb_compactions_failed_total[3h]) > 0
            for: 1h
            labels:
              severity: warning
            annotations:
              summary: "Prometheus TSDB compactions are failing"
              description: "Prometheus TSDB has had {{ $value }} failed compactions in the last 3 hours."

          - alert: PrometheusStorageAlmostFull
            expr: (prometheus_tsdb_storage_blocks_bytes / (100 * 1024 * 1024 * 1024)) * 100 > 80
            for: 30m
            labels:
              severity: warning
            annotations:
              summary: "Prometheus storage is almost full"
              description: "Prometheus TSDB storage is at {{ $value | printf \"%.1f\" }}% of 100GB limit."

          - alert: LokiDown
            expr: up{job=~".*loki.*"} == 0
            for: 5m
            labels:
              severity: critical
            annotations:
              summary: "Loki is down"
              description: "Loki has been unreachable for more than 5 minutes."

          - alert: AlloyDown
            expr: up{job="alloy"} == 0
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Alloy is down"
              description: "Alloy instance {{ $labels.instance }} has been unreachable for more than 5 minutes."

      - name: traefik-alerts
        rules:
          - alert: TraefikDown
            expr: up{job="traefik"} == 0
            for: 2m
            labels:
              severity: critical
            annotations:
              summary: "Traefik is down"
              description: "Traefik instance on {{ $labels.node }} has been unreachable for more than 2 minutes."

          - alert: TraefikHighErrorRate
            expr: sum(rate(traefik_service_requests_total{code=~"5.."}[5m])) / sum(rate(traefik_service_requests_total[5m])) * 100 > 5
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Traefik high 5xx error rate"
              description: "Traefik 5xx error rate is {{ $value | printf \"%.2f\" }}% (threshold: 5%)."

      - name: cilium-alerts
        rules:
          - alert: CiliumAgentDown
            expr: up{job="cilium-agent"} == 0
            for: 2m
            labels:
              severity: critical
            annotations:
              summary: "Cilium agent is down"
              description: "Cilium agent on {{ $labels.node }} has been unreachable for more than 2 minutes."

          - alert: CiliumEndpointNotReady
            expr: cilium_endpoint_state{endpoint_state="not-ready"} > 0
            for: 15m
            labels:
              severity: warning
            annotations:
              summary: "Cilium endpoints not ready on {{ $labels.node }}"
              description: "{{ $value }} Cilium endpoints are in not-ready state on {{ $labels.node }}."

      - name: argocd-alerts
        rules:
          - alert: ArgoCDDown
            expr: up{job="argocd"} == 0
            for: 5m
            labels:
              severity: critical
            annotations:
              summary: "ArgoCD component {{ $labels.component }} is down"
              description: "ArgoCD {{ $labels.component }} ({{ $labels.instance }}) has been unreachable for more than 5 minutes."

          - alert: ArgoCDAppOutOfSync
            expr: argocd_app_info{sync_status!="Synced"} == 1
            for: 15m
            labels:
              severity: warning
            annotations:
              summary: "ArgoCD app {{ $labels.name }} is out of sync"
              description: "Application {{ $labels.name }} in project {{ $labels.project }} has been out of sync for more than 15 minutes."

          - alert: ArgoCDAppDegraded
            expr: argocd_app_info{health_status=~"Degraded|Missing"} == 1
            for: 10m
            labels:
              severity: warning
            annotations:
              summary: "ArgoCD app {{ $labels.name }} is {{ $labels.health_status }}"
              description: "Application {{ $labels.name }} health status is {{ $labels.health_status }} for more than 10 minutes."

      - name: harbor-alerts
        rules:
          - alert: HarborDown
            expr: up{job="harbor"} == 0
            for: 5m
            labels:
              severity: critical
            annotations:
              summary: "Harbor {{ $labels.component }} is down"
              description: "Harbor {{ $labels.component }} has been unreachable for more than 5 minutes."

      - name: keycloak-alerts
        rules:
          - alert: KeycloakDown
            expr: up{job="keycloak"} == 0
            for: 5m
            labels:
              severity: critical
            annotations:
              summary: "Keycloak is down"
              description: "Keycloak instance {{ $labels.instance }} has been unreachable for more than 5 minutes."

      - name: mattermost-alerts
        rules:
          - alert: MattermostDown
            expr: up{job="mattermost"} == 0
            for: 5m
            labels:
              severity: critical
            annotations:
              summary: "Mattermost is down"
              description: "Mattermost instance {{ $labels.instance }} has been unreachable for more than 5 minutes."

      - name: security-alerts
        rules:
          - alert: KeycloakBruteForceDetected
            expr: increase(keycloak_login_errors_total{error="user_not_found"}[5m]) > 10
            for: 2m
            labels:
              severity: warning
            annotations:
              summary: "Potential brute force attack on Keycloak"
              description: "More than 10 failed login attempts (user not found) in 5 minutes on {{ $labels.instance }}."

          - alert: KeycloakHighLoginFailures
            expr: rate(keycloak_login_errors_total[5m]) / (rate(keycloak_logins_total[5m]) + rate(keycloak_login_errors_total[5m])) > 0.3
            for: 10m
            labels:
              severity: warning
            annotations:
              summary: "High Keycloak login failure rate"
              description: "Login failure rate is {{ $value | printf \"%.1f\" }}% for more than 10 minutes."

          - alert: UnusualPodNetworkTraffic
            expr: sum by (namespace, pod) (rate(container_network_transmit_bytes_total[5m])) > 50000000
            for: 15m
            labels:
              severity: warning
            annotations:
              summary: "Unusual outbound network traffic from {{ $labels.namespace }}/{{ $labels.pod }}"
              description: "Pod {{ $labels.namespace }}/{{ $labels.pod }} is transmitting {{ $value | humanize }}B/s outbound (>50MB/s for 15min)."

          - alert: PrivilegedContainerRunning
            expr: kube_pod_spec_volumes_persistentvolumeclaims_info == 0 and on(namespace, pod) kube_pod_container_info{container!=""}
            for: 0m
            labels:
              severity: info

          - alert: APIServerUnusualRequestRate
            expr: sum(rate(apiserver_request_total{verb!~"GET|LIST|WATCH"}[5m])) > 100
            for: 10m
            labels:
              severity: warning
            annotations:
              summary: "Unusual Kubernetes API write request rate"
              description: "API server is receiving {{ $value | printf \"%.0f\" }} non-read requests/s (>100 for 10min). Possible automated attack or misconfigured controller."

          - alert: HighDNSQueryRate
            expr: sum(rate(coredns_dns_requests_total[5m])) by (server) > 5000
            for: 10m
            labels:
              severity: warning
            annotations:
              summary: "Unusually high DNS query rate"
              description: "CoreDNS server {{ $labels.server }} receiving {{ $value | printf \"%.0f\" }} queries/s. Possible DNS tunneling or exfiltration."

          - alert: SecretAccessSpike
            expr: sum(rate(apiserver_request_total{resource="secrets", verb=~"get|list"}[5m])) > 50
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Spike in Kubernetes secret access"
              description: "{{ $value | printf \"%.0f\" }} secret access requests/s detected. Possible credential harvesting."
