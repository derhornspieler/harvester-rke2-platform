#!/usr/bin/env bash
set -euo pipefail

# --- Colors & Logging ---
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

log_info()  { echo -e "${BLUE}[INFO]${NC}  $*"; }
log_ok()    { echo -e "${GREEN}[OK]${NC}    $*"; }
log_warn()  { echo -e "${YELLOW}[WARN]${NC}  $*"; }
log_error() { echo -e "${RED}[ERROR]${NC} $*" >&2; }

# --- Constants ---
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
HARVESTER_KUBECONFIG="${SCRIPT_DIR}/kubeconfig-harvester.yaml"
TF_NAMESPACE="terraform-state"
KUBECTL="kubectl --kubeconfig=${HARVESTER_KUBECONFIG}"

# Files to store as secrets (parallel arrays: filename -> secret name)
SECRET_FILENAMES=("terraform.tfvars" "kubeconfig-harvester.yaml" "kubeconfig-harvester-cloud-cred.yaml" "harvester-cloud-provider-kubeconfig" "vault-init.json" "root-ca.pem" "root-ca-key.pem")
SECRET_NAMES=("terraform-tfvars" "kubeconfig-harvester" "kubeconfig-harvester-cloud-cred" "harvester-cloud-provider-kubeconfig" "vault-init" "root-ca-cert" "root-ca-key")

# --- Helper Functions ---

# Extract a quoted tfvars value by variable name
_get_tfvar_value() {
  awk -F'"' "/^${1}[[:space:]]/ {print \$2}" "${SCRIPT_DIR}/terraform.tfvars" 2>/dev/null || echo ""
}

check_prerequisites() {
  local missing=()
  for cmd in kubectl terraform jq; do
    if ! command -v "$cmd" &>/dev/null; then
      missing+=("$cmd")
    fi
  done
  if [[ ${#missing[@]} -gt 0 ]]; then
    log_error "Missing required tools: ${missing[*]}"
    exit 1
  fi
  log_ok "Prerequisites found: kubectl, terraform, jq"
}

check_connectivity() {
  if [[ ! -f "$HARVESTER_KUBECONFIG" ]]; then
    log_info "Harvester kubeconfig not found, extracting from ~/.kube/config (context: harvester)..."
    if kubectl config view --minify --context=harvester --raw > "$HARVESTER_KUBECONFIG" 2>/dev/null && [[ -s "$HARVESTER_KUBECONFIG" ]]; then
      chmod 600 "$HARVESTER_KUBECONFIG"
      log_ok "Harvester kubeconfig extracted to ${HARVESTER_KUBECONFIG}"
    else
      rm -f "$HARVESTER_KUBECONFIG"
      log_error "Harvester kubeconfig not found: ${HARVESTER_KUBECONFIG}"
      log_error "Add a 'harvester' context to ~/.kube/config or place the file manually"
      exit 1
    fi
  fi
  if ! $KUBECTL cluster-info &>/dev/null; then
    log_error "Cannot connect to Harvester cluster via ${HARVESTER_KUBECONFIG}"
    exit 1
  fi
  log_ok "Harvester cluster is reachable"
}

ensure_namespace() {
  if ! $KUBECTL get namespace "$TF_NAMESPACE" &>/dev/null; then
    log_info "Creating namespace ${TF_NAMESPACE}..."
    $KUBECTL create namespace "$TF_NAMESPACE"
    log_ok "Namespace ${TF_NAMESPACE} created"
  else
    log_ok "Namespace ${TF_NAMESPACE} exists"
  fi
}

clear_stale_lock() {
  cd "$SCRIPT_DIR"
  # Try a quick plan to see if the state is locked
  local output
  output=$(terraform plan -input=false -no-color 2>&1 || true)
  if echo "$output" | grep -q "Error acquiring the state lock"; then
    local lock_id
    lock_id=$(echo "$output" | grep 'ID:' | head -1 | awk '{print $2}')
    if [[ -n "$lock_id" ]]; then
      log_warn "Terraform state is locked (stale lock from a previous run)"
      log_info "Lock ID: ${lock_id}"
      log_info "Auto-unlocking..."
      if terraform force-unlock -force "$lock_id" 2>/dev/null; then
        log_ok "State lock cleared"
      else
        log_error "Failed to clear state lock. Run: terraform force-unlock -force ${lock_id}"
        return 1
      fi
    fi
  fi
}

check_rbac() {
  local ok=true
  for action in "create secrets" "create leases"; do
    if ! $KUBECTL auth can-i $action -n "$TF_NAMESPACE" &>/dev/null; then
      log_error "Insufficient permissions: cannot ${action} in ${TF_NAMESPACE}"
      ok=false
    fi
  done
  if [[ "$ok" != "true" ]]; then
    exit 1
  fi
  log_ok "RBAC permissions verified (secrets + leases)"
}

push_secrets() {
  log_info "Pushing local files to K8s secrets in ${TF_NAMESPACE}..."
  local pushed=0
  for i in "${!SECRET_FILENAMES[@]}"; do
    local file="${SECRET_FILENAMES[$i]}"
    local secret_name="${SECRET_NAMES[$i]}"
    local filepath="${SCRIPT_DIR}/${file}"
    if [[ -f "$filepath" ]]; then
      $KUBECTL create secret generic "$secret_name" \
        --from-file="${file}=${filepath}" \
        --namespace="$TF_NAMESPACE" \
        --dry-run=client -o yaml | $KUBECTL apply -f -
      log_ok "  ${secret_name} <- ${file}"
      pushed=$((pushed + 1))
    else
      log_warn "  Skipping ${file} (not found)"
    fi
  done
  log_ok "Pushed ${pushed} secret(s)"
}

pull_secrets() {
  log_info "Pulling secrets from ${TF_NAMESPACE} to local files..."
  local pulled=0
  for i in "${!SECRET_FILENAMES[@]}"; do
    local file="${SECRET_FILENAMES[$i]}"
    local secret_name="${SECRET_NAMES[$i]}"
    local filepath="${SCRIPT_DIR}/${file}"
    if $KUBECTL get secret "$secret_name" -n "$TF_NAMESPACE" &>/dev/null; then
      local tmpfile
      tmpfile=$(mktemp)
      $KUBECTL get secret "$secret_name" -n "$TF_NAMESPACE" -o json \
        | jq -r ".data[\"${file}\"]" | base64 -d > "$tmpfile"
      mv "$tmpfile" "$filepath"
      chmod 600 "$filepath"
      log_ok "  ${file} <- ${secret_name}"
      pulled=$((pulled + 1))
    else
      log_warn "  Skipping ${secret_name} (not found in cluster)"
    fi
  done
  log_ok "Pulled ${pulled} secret(s)"
}

# --- Commands ---

cmd_init() {
  log_info "Initializing Terraform with Kubernetes backend..."
  echo

  check_prerequisites
  check_connectivity
  ensure_namespace
  check_rbac
  echo

  push_secrets
  echo

  log_info "Running terraform init -migrate-state..."
  cd "$SCRIPT_DIR"
  terraform init -migrate-state
  echo

  log_ok "Initialization complete. State is now stored in K8s secret: tfstate-default-rke2-cluster"
}

cmd_push_secrets() {
  check_connectivity
  ensure_namespace
  push_secrets
}

cmd_pull_secrets() {
  check_connectivity
  pull_secrets
}

cmd_apply() {
  check_connectivity
  pull_secrets
  echo

  cd "$SCRIPT_DIR"
  if [[ ! -d .terraform ]] || ! terraform validate -no-color &>/dev/null; then
    log_info "Initializing Terraform backend..."
    if ! terraform init -input=false 2>&1; then
      log_warn "Backend init failed — retrying with -reconfigure (backend config hash may be stale)..."
      terraform init -input=false -reconfigure
    fi
    echo
  fi

  clear_stale_lock

  # Generate dated plan file
  local plan_file="tfplan_$(date +%Y%m%d_%H%M%S)"
  log_info "Running: terraform plan -out=${plan_file}"
  terraform plan -out="$plan_file"
  echo

  log_info "Running: terraform apply ${plan_file}"
  local tf_exit=0
  terraform apply "$plan_file" || tf_exit=$?
  rm -f "$plan_file"

  # Terraform may exit 1 with "Error releasing the state lock" even when
  # apply succeeds (K8s backend lock timeout during long cluster creation).
  # Check if resources were actually created before failing.
  if [[ $tf_exit -ne 0 ]]; then
    if terraform state list 2>/dev/null | grep -q "rancher2_cluster_v2"; then
      log_warn "Terraform exited $tf_exit but resources were created — continuing"
    else
      log_error "Terraform apply failed (exit $tf_exit)"
      return $tf_exit
    fi
  fi
  echo

  # Always push secrets after successful apply
  log_info "Pushing secrets to Harvester after successful apply..."
  ensure_namespace
  push_secrets
}

cmd_terraform() {
  check_connectivity
  pull_secrets
  echo
  log_info "Running: terraform $*"
  cd "$SCRIPT_DIR"
  terraform "$@"
}

# ---------------------------------------------------------------------------
# Post-destroy cleanup: remove orphaned VMs, VMIs, DataVolumes, and PVCs
# from Harvester that terraform destroy leaves behind.
#
# Why this is needed:
#   terraform destroy deletes rancher2_cluster_v2 → Rancher starts async
#   VM teardown via CAPI → Terraform then deletes the cloud credential →
#   Harvester node driver loses access → VMs get stuck with finalizers →
#   VM disk PVCs accumulate on every destroy/recreate cycle.
# ---------------------------------------------------------------------------
post_destroy_cleanup() {
  local vm_namespace="$1"
  local cluster_name="$2"

  if [[ -z "$vm_namespace" || -z "$cluster_name" ]]; then
    log_warn "Could not determine VM namespace or cluster name — skipping Harvester cleanup"
    return 0
  fi

  echo
  log_info "Post-destroy cleanup: checking for orphaned resources in Harvester namespace '${vm_namespace}'..."

  # --- Clear stuck CAPI finalizers on Rancher management cluster ---
  local rancher_url rancher_token
  rancher_url=$(_get_tfvar_value rancher_url)
  rancher_token=$(_get_tfvar_value rancher_token)

  if [[ -n "$rancher_url" && -n "$rancher_token" ]]; then
    local auth_header="Authorization: Bearer ${rancher_token}"

    # Clear HarvesterMachine finalizers FIRST (they're the root)
    # NOTE: Use PATCH with merge-patch, NOT GET+jq+PUT — JSON responses contain
    #       binary cloud-init data that breaks jq parsing
    local hm_names
    hm_names=$(curl -sk -H "$auth_header" \
      "${rancher_url}/v1/rke-machine.cattle.io.harvestermachines" 2>/dev/null \
      | jq -r '.data[]? | select(.metadata.deletionTimestamp != null) | .metadata.name' 2>/dev/null || true)

    if [[ -n "$hm_names" ]]; then
      log_warn "Clearing stuck HarvesterMachine finalizers on Rancher..."
      while IFS= read -r name; do
        [[ -z "$name" ]] && continue
        curl -sk -X PATCH -H "$auth_header" \
          -H "Content-Type: application/merge-patch+json" \
          "${rancher_url}/v1/rke-machine.cattle.io.harvestermachines/fleet-default/${name}" \
          -d '{"metadata":{"finalizers":[]}}' > /dev/null 2>&1 || true
        log_info "  Patched: ${name}"
      done <<< "$hm_names"
      sleep 5
    fi

    # Clear CAPI Machine finalizers (cascade from HarvesterMachines)
    local capi_names
    capi_names=$(curl -sk -H "$auth_header" \
      "${rancher_url}/v1/cluster.x-k8s.io.machines" 2>/dev/null \
      | jq -r '.data[]? | select(.metadata.deletionTimestamp != null) | .metadata.name' 2>/dev/null || true)

    if [[ -n "$capi_names" ]]; then
      log_warn "Clearing stuck CAPI Machine finalizers on Rancher..."
      while IFS= read -r name; do
        [[ -z "$name" ]] && continue
        curl -sk -X PATCH -H "$auth_header" \
          -H "Content-Type: application/merge-patch+json" \
          "${rancher_url}/v1/cluster.x-k8s.io.machines/fleet-default/${name}" \
          -d '{"metadata":{"finalizers":[]}}' > /dev/null 2>&1 || true
        log_info "  Patched: ${name}"
      done <<< "$capi_names"
      sleep 5
    fi

    # Clear provisioning cluster finalizers (if cluster is stuck deleting)
    local cluster_dt
    cluster_dt=$(curl -sk -H "$auth_header" \
      "${rancher_url}/v1/provisioning.cattle.io.clusters/fleet-default/${cluster_name}" 2>/dev/null \
      | jq -r '.metadata.deletionTimestamp // empty' 2>/dev/null || true)

    if [[ -n "$cluster_dt" ]]; then
      log_warn "Clearing stuck cluster finalizers on Rancher..."
      curl -sk -X PATCH -H "$auth_header" \
        -H "Content-Type: application/merge-patch+json" \
        "${rancher_url}/v1/provisioning.cattle.io.clusters/fleet-default/${cluster_name}" \
        -d '{"metadata":{"finalizers":[]}}' > /dev/null 2>&1 || true
      log_info "  Patched cluster: ${cluster_name}"
      sleep 5
    fi
  fi

  # --- Wait for VMs to be deleted (async CAPI teardown) ---
  local timeout=300 interval=10 elapsed=0
  while [[ $elapsed -lt $timeout ]]; do
    local vm_count
    vm_count=$($KUBECTL get virtualmachines.kubevirt.io -n "$vm_namespace" --no-headers 2>/dev/null \
      | grep -c "^${cluster_name}-" || true)

    if [[ "$vm_count" -eq 0 ]]; then
      log_ok "All cluster VMs deleted from Harvester"
      break
    fi

    log_info "  ${vm_count} VM(s) still deleting (${elapsed}s/${timeout}s)..."
    sleep "$interval"
    elapsed=$((elapsed + interval))
  done

  # --- Remove stuck finalizers on VMs ---
  local stuck_vms
  stuck_vms=$($KUBECTL get virtualmachines.kubevirt.io -n "$vm_namespace" \
    --no-headers -o name 2>/dev/null | grep "${cluster_name}" || true)

  if [[ -n "$stuck_vms" ]]; then
    log_warn "Removing stuck finalizers from remaining VMs..."
    while IFS= read -r vm; do
      [[ -z "$vm" ]] && continue
      $KUBECTL patch "$vm" -n "$vm_namespace" --type=merge \
        -p '{"metadata":{"finalizers":null}}' 2>/dev/null || true
      log_info "  Patched: ${vm}"
    done <<< "$stuck_vms"
    sleep 10
  fi

  # --- Remove stuck VMIs ---
  local stuck_vmis
  stuck_vmis=$($KUBECTL get virtualmachineinstances.kubevirt.io -n "$vm_namespace" \
    --no-headers -o name 2>/dev/null | grep "${cluster_name}" || true)

  if [[ -n "$stuck_vmis" ]]; then
    log_warn "Removing stuck VirtualMachineInstances..."
    while IFS= read -r vmi; do
      [[ -z "$vmi" ]] && continue
      $KUBECTL patch "$vmi" -n "$vm_namespace" --type=merge \
        -p '{"metadata":{"finalizers":null}}' 2>/dev/null || true
      $KUBECTL delete "$vmi" -n "$vm_namespace" --wait=false 2>/dev/null || true
    done <<< "$stuck_vmis"
    sleep 5
  fi

  # --- Delete ALL DataVolumes in namespace (workload PVCs from cluster services) ---
  local all_dvs
  all_dvs=$($KUBECTL get datavolumes.cdi.kubevirt.io -n "$vm_namespace" \
    --no-headers -o name 2>/dev/null || true)

  if [[ -n "$all_dvs" ]]; then
    local dv_count
    dv_count=$(echo "$all_dvs" | wc -l | tr -d ' ')
    log_warn "Found ${dv_count} DataVolume(s) in namespace — deleting..."
    while IFS= read -r dv; do
      [[ -z "$dv" ]] && continue
      $KUBECTL patch "$dv" -n "$vm_namespace" --type=merge \
        -p '{"metadata":{"finalizers":null}}' 2>/dev/null || true
      $KUBECTL delete "$dv" -n "$vm_namespace" --wait=false 2>/dev/null || true
    done <<< "$all_dvs"
    sleep 5
  fi

  # --- Delete ALL PVCs in namespace (VM disks + workload volumes) ---
  # The namespace is dedicated to this cluster, so all PVCs are safe to remove.
  local all_pvcs
  all_pvcs=$($KUBECTL get pvc -n "$vm_namespace" --no-headers -o name 2>/dev/null || true)

  if [[ -n "$all_pvcs" ]]; then
    local pvc_count
    pvc_count=$(echo "$all_pvcs" | wc -l | tr -d ' ')
    log_warn "Found ${pvc_count} PVC(s) in namespace — deleting..."
    while IFS= read -r pvc; do
      [[ -z "$pvc" ]] && continue
      $KUBECTL patch "$pvc" -n "$vm_namespace" --type=merge \
        -p '{"metadata":{"finalizers":null}}' 2>/dev/null || true
      $KUBECTL delete "$pvc" -n "$vm_namespace" --wait=false 2>/dev/null || true
      log_info "  Deleted: ${pvc}"
    done <<< "$all_pvcs"
    log_ok "All PVCs cleaned up"
  else
    log_ok "No PVCs found"
  fi

  # --- Summary ---
  local remaining
  remaining=$($KUBECTL get pvc -n "$vm_namespace" --no-headers 2>/dev/null | wc -l | tr -d ' ')
  if [[ "$remaining" -gt 0 ]]; then
    log_warn "${remaining} PVC(s) still in namespace '${vm_namespace}' (deletion may be in progress)"
    $KUBECTL get pvc -n "$vm_namespace" --no-headers 2>/dev/null | while read -r line; do
      log_info "  ${line}"
    done
  else
    log_ok "Namespace '${vm_namespace}' is clean"
  fi
}

cmd_destroy() {
  check_connectivity
  pull_secrets
  echo

  # Ensure backend is initialized (may be missing after fresh clone or .terraform cleanup)
  cd "$SCRIPT_DIR"
  if [[ ! -d .terraform ]] || ! terraform validate -no-color &>/dev/null; then
    log_info "Initializing Terraform backend..."
    if ! terraform init -input=false 2>&1; then
      log_warn "Backend init failed — retrying with -reconfigure (backend config hash may be stale)..."
      terraform init -input=false -reconfigure
    fi
    echo
  fi

  clear_stale_lock

  # Capture VM namespace and cluster name BEFORE destroy removes Terraform state
  local vm_namespace cluster_name
  vm_namespace=$(_get_tfvar_value vm_namespace)
  cluster_name=$(_get_tfvar_value cluster_name)

  log_info "Running: terraform destroy $*"
  cd "$SCRIPT_DIR"
  terraform destroy "$@"

  # Clean up orphaned Harvester resources that terraform destroy leaves behind
  post_destroy_cleanup "$vm_namespace" "$cluster_name"

  # Push secrets after successful destroy (state is now empty but secrets persist)
  echo
  log_info "Pushing secrets to Harvester after successful destroy..."
  ensure_namespace
  push_secrets
}

# --- Main ---

usage() {
  cat <<EOF
Usage: $(basename "$0") <command> [args...]

Commands:
  init            Initialize K8s backend (create namespace, push secrets, migrate state)
  apply           Pull secrets → plan (saved) → apply → push secrets to Harvester
  destroy         Destroy cluster + cleanup orphaned VMs/PVCs + push secrets
  push-secrets    Push local tfvars + kubeconfigs to K8s secrets
  pull-secrets    Pull tfvars + kubeconfigs from K8s secrets to local files
  <any>           Pull secrets, then run 'terraform <any>' (e.g., plan, output)

Examples:
  $(basename "$0") init
  $(basename "$0") apply
  $(basename "$0") destroy -auto-approve
  $(basename "$0") plan
  $(basename "$0") push-secrets
EOF
}

if [[ $# -lt 1 ]]; then
  usage
  exit 1
fi

COMMAND="$1"
shift

case "$COMMAND" in
  init)
    cmd_init
    ;;
  apply)
    cmd_apply
    ;;
  push-secrets)
    cmd_push_secrets
    ;;
  pull-secrets)
    cmd_pull_secrets
    ;;
  destroy)
    cmd_destroy "$@"
    ;;
  -h|--help|help)
    usage
    ;;
  *)
    cmd_terraform "$COMMAND" "$@"
    ;;
esac
